#!/usr/bin/env python3
"""
åŸºäºHARLæ¡†æ¶çš„åˆ›æ–°ç‚¹ä¸€ç²¾å‡†éªŒè¯è„šæœ¬ (æœ€ç»ˆå®Œæ•´ä¿®æ­£ç‰ˆ)

åŠŸèƒ½:
- å®ç°ç§‘å­¦çš„å®éªŒæ¡†æ¶ï¼Œæ”¯æŒåŸºå‡†ã€æ¶ˆèå’Œå®Œæ•´å¢å¼ºæ¨¡å¼ã€‚
- ä¿®æ­£äº†æ‰€æœ‰å·²çŸ¥çš„APIè°ƒç”¨å’Œæ•°æ®ç±»å‹é”™è¯¯ã€‚
- å®Œæ•´é›†æˆäº†å®šæ€§åˆ†æï¼ˆæ³¨æ„åŠ›æƒé‡ã€t-SNEï¼‰çš„æ•°æ®æ”¶é›†ä¸å¯è§†åŒ–é€»è¾‘ã€‚
"""

import os
import sys
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.tensorboard import SummaryWriter
import yaml
import json
import time
from collections import deque
import matplotlib.pyplot as plt
from gym import spaces

# æ·»åŠ HARLè·¯å¾„
# å‡è®¾æ­¤è„šæœ¬åœ¨é¡¹ç›®æ ¹ç›®å½•è¿è¡Œï¼Œæˆ–è€…å·²é€šè¿‡å…¶ä»–æ–¹å¼è®¾ç½®è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

# å¯¼å…¥HARLæ¡†æ¶ç»„ä»¶
from harl.algorithms.actors.hasac import HASAC
from harl.algorithms.critics.soft_twin_continuous_q_critic import SoftTwinContinuousQCritic
from harl.models.policy_models.transformer_policy import TransformerEnhancedPolicy
from harl.models.base.transformer import TransformerEncoder, HistoryBuffer
from harl.utils.contrastive_learning import EnhancedContrastiveLoss
from harl.common.buffers.off_policy_buffer_ep import OffPolicyBufferEP

# å¯¼å…¥MEC-V2Xç¯å¢ƒ
from hasac_flow_mec_v2x_env import MECVehicularEnvironment


class HARLBasedInnovation1Validator:
    """åŸºäºHARLæ¡†æ¶çš„åˆ›æ–°ç‚¹ä¸€éªŒè¯å™¨"""
    
    def __init__(self, config_path="harl_innovation1_config.yaml"):
        """
        åˆå§‹åŒ–éªŒè¯å™¨
        
        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        """
        self.config_path = config_path
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # åŠ è½½é…ç½®
        self.config = self._load_config()
        
        # è·å–å®éªŒæ¨¡å¼
        self.mode = self.config.get('experiment_mode', 'enhanced')
        self.ablation_mode = self.config.get('ablation_mode', 'full')
        
        # æ ¹æ®æ¨¡å¼å†³å®šæ˜¯å¦å¯ç”¨åˆ›æ–°
        if self.mode == 'baseline':
            self.use_transformer_flag = False
            self.use_contrastive_learning_flag = False
            print("\nğŸ”¬ è¿è¡ŒåŸºå‡†æ¨¡å¼ (BASELINE)ï¼šVanilla HASACï¼Œæ— ä»»ä½•å¢å¼º")
        else:
            if self.ablation_mode == 'transformer_only':
                self.use_transformer_flag = True
                self.use_contrastive_learning_flag = False
                print("\nğŸ§ª è¿è¡Œæ¶ˆèç ”ç©¶æ¨¡å¼ï¼šä»…å¯ç”¨Transformerå¢å¼º")
            elif self.ablation_mode == 'contrastive_only':
                self.use_transformer_flag = True
                self.use_contrastive_learning_flag = True
                print("\nğŸ§ª è¿è¡Œæ¶ˆèç ”ç©¶æ¨¡å¼ï¼šå¯ç”¨å¯¹æ¯”å­¦ä¹ ï¼ˆåŸºäºTransformerè¡¨å¾ï¼‰")
            else:  # 'full'æ¨¡å¼
                self.use_transformer_flag = self.config.get('use_transformer', True)
                self.use_contrastive_learning_flag = self.config.get('use_contrastive_learning', True)
                print("\nğŸš€ è¿è¡Œå®Œæ•´å¢å¼ºæ¨¡å¼ï¼šå¯ç”¨å…¨éƒ¨åˆ›æ–°ç‚¹1åŠŸèƒ½")
        
        # åˆ›å»ºå¸¦æ¨¡å¼åç§°çš„æ—¥å¿—ç›®å½•
        timestamp = int(time.time())
        self.log_dir = f"logs/{self.mode}_{self.ablation_mode}_run_{timestamp}"
        os.makedirs(self.log_dir, exist_ok=True)
        
        # åˆå§‹åŒ–TensorBoard
        self.writer = SummaryWriter(self.log_dir)
        
        # åˆ›å»ºMEC-V2Xç¯å¢ƒ
        self.env = MECVehicularEnvironment(
            num_vehicles=self.config['num_agents'],
            num_rsus=self.config['num_rsus'],
            map_size=self.config['map_size'],
            max_history_length=self.config['max_seq_length']
        )
        
        # è·å–ç¯å¢ƒä¿¡æ¯
        self.num_agents = self.env.num_vehicles
        self.obs_dim_single = self.env.observation_space['vehicle_0'].shape[0]
        self.action_dim_single = self.env.action_space[0].shape[0]  # FIX: ä½¿ç”¨æ•´æ•°ç´¢å¼•è®¿é—®Tupleç±»å‹çš„action_space

        # åˆå§‹åŒ–ç®—æ³•ç»„ä»¶
        self.agents = self._create_hasac_agents()
        self.critics = self._create_critics()
        self.buffer = self._create_buffer()
        
        # æ€§èƒ½æŒ‡æ ‡
        self.metrics = {
            'episode_rewards': [], 'episode_lengths': [],
            'transformer_effectiveness': [], 'contrastive_loss_values': [],
            'attention_weights': [], 'state_embeddings': []
        }
        
        print(f"âœ“ HARL-based Innovation 1 Validator initialized")
        print(f"âœ“ Device: {self.device}")
        print(f"âœ“ Agents: {self.num_agents}, Obs dim: {self.obs_dim_single}, Action dim: {self.action_dim_single}")
        print(f"âœ“ Experiment mode: {self.mode}, Ablation mode: {self.ablation_mode}")
        print(f"âœ“ Logs will be saved to: {self.log_dir}")
    
    def _load_config(self):
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        default_config = {
            'num_agents': 5, 'num_rsus': 2, 'map_size': 1000.0,
            'max_episodes': 1000, 'max_steps': 200, 'batch_size': 32,
            'lr': 3e-4, 'critic_lr': 3e-4, 'polyak': 0.995, 'alpha': 0.2, 'gamma': 0.99, 'alpha_lr': 3e-4,
            'buffer_size': 100000, 'start_steps': 5000, 'update_after': 1000, 'update_every': 50,
            'experiment_mode': 'enhanced', 'ablation_mode': 'full',
            'use_transformer': True, 'max_seq_length': 50, 'transformer_d_model': 256,
            'transformer_nhead': 8, 'transformer_num_layers': 4, 'transformer_dim_feedforward': 512,
            'transformer_dropout': 0.1, 'use_contrastive_learning': True,
            'contrastive_temperature': 0.1, 'similarity_threshold': 0.8,
            'temporal_weight': 0.1, 'contrastive_loss_weight': 0.1,
            'hidden_size': 256, 'activation': 'relu', 'final_activation': 'identity',
            'state_type': 'EP', 'save_attention_weights': True,
            'save_state_embeddings': True, 'visualization_interval': 100,
            'use_proper_time_limits': True,
            'n_step': 1,
            'n_rollout_threads': 1,
            'episode_length': 200
        }
        if os.path.exists(self.config_path):
            with open(self.config_path, 'r') as f:
                file_config = yaml.safe_load(f)
                if file_config:
                    default_config.update(file_config)
        else:
            with open(self.config_path, 'w') as f:
                yaml.dump(default_config, f, default_flow_style=False)
        return default_config
    
    def _create_hasac_agents(self):
        """åˆ›å»ºHASACæ™ºèƒ½ä½“"""
        agents = []
        obs_space = self.env.observation_space['vehicle_0']
        action_space = self.env.action_space[0]  # FIX: ä½¿ç”¨æ•´æ•°ç´¢å¼•è®¿é—®Tupleç±»å‹çš„action_space
        
        hasac_args = {
            'lr': self.config['lr'], 'polyak': self.config['polyak'], 'alpha': self.config['alpha'],
            'use_transformer': self.use_transformer_flag,
            'use_contrastive_learning': self.use_contrastive_learning_flag,
            'max_seq_length': self.config['max_seq_length'],
            'transformer_d_model': self.config['transformer_d_model'],
            'transformer_nhead': self.config['transformer_nhead'],
            'transformer_num_layers': self.config['transformer_num_layers'],
            'transformer_dim_feedforward': self.config['transformer_dim_feedforward'],
            'transformer_dropout': self.config['transformer_dropout'],
            'contrastive_temperature': self.config['contrastive_temperature'],
            'similarity_threshold': self.config['similarity_threshold'],
            'temporal_weight': self.config['temporal_weight'],
            'hidden_sizes': [self.config['hidden_size'], self.config['hidden_size']],
            'activation_func': self.config['activation'],
            'final_activation_func': self.config['final_activation']
        }
        for i in range(self.num_agents):
            agent = HASAC(hasac_args, obs_space, action_space, self.device)
            agents.append(agent)
        return agents
    
    def _create_critics(self):
        """åˆ›å»ºè½¯åŒQè¯„è®ºå®¶ç½‘ç»œ"""
        critics = []
        # FIX: Tupleå¯¹è±¡æ²¡æœ‰valuesæ–¹æ³•ï¼Œç›´æ¥ä½¿ç”¨spaceså±æ€§
        action_space_list = list(self.env.action_space.spaces)
        multi_agent_action_space_tuple = spaces.Tuple(action_space_list)
        
        global_obs_space = self.env.share_observation_space['vehicle_0']

        critic_args = {
            'critic_lr': self.config.get('critic_lr', self.config['lr']), 
            'polyak': self.config['polyak'], 
            'alpha': self.config['alpha'],
            'gamma': self.config['gamma'],
            'alpha_lr': self.config.get('alpha_lr', 3e-4),
            'hidden_sizes': [self.config['hidden_size'], self.config['hidden_size']],
            'activation_func': self.config['activation'],
            'final_activation_func': self.config['final_activation'],
            'auto_alpha': False,
            'use_policy_active_masks': False, 
            'use_huber_loss': True, 
            'huber_delta': 10.0,
            'use_proper_time_limits': self.config.get('use_proper_time_limits', True)
        }
        state_type = self.config.get('state_type', 'EP')
        
        for i in range(self.num_agents):
            critic = SoftTwinContinuousQCritic(
                critic_args, global_obs_space, multi_agent_action_space_tuple, # Pass the Tuple space
                self.num_agents, state_type, self.device
            )
            critics.append(critic)
        return critics
    
    def _create_buffer(self):
        """åˆ›å»ºç»éªŒå›æ”¾ç¼“å†²åŒº"""
        buffer_args = {
            'buffer_size': self.config['buffer_size'],
            'batch_size': self.config['batch_size'],
            'gamma': self.config['gamma'],
            'n_step': self.config['n_step'],
            'n_rollout_threads': self.config['n_rollout_threads'],
            'episode_length': self.config['episode_length'],
            'device': self.device,
            'n_agents': self.num_agents # Add n_agents to the args dictionary
        }
        
        # åˆ›å»ºæ¯ä¸ªæ™ºèƒ½ä½“çš„è§‚å¯Ÿç©ºé—´å’ŒåŠ¨ä½œç©ºé—´åˆ—è¡¨
        obs_spaces = []
        act_spaces = []
        for i in range(self.num_agents):
            obs_spaces.append(self.env.observation_space[f'vehicle_{i}'])
            act_spaces.append(self.env.action_space[i])  # ä½¿ç”¨ç´¢å¼•è®¿é—®Tupleç±»å‹çš„action_space
        
        # è·å–å…±äº«è§‚å¯Ÿç©ºé—´
        share_obs_space = self.env.share_observation_space['vehicle_0']
        
        try:
            # å°è¯•åˆ›å»ºç¼“å†²åŒº
            buffer = OffPolicyBufferEP(
                buffer_args, share_obs_space, self.num_agents, obs_spaces, act_spaces
            )
            print("âœ“ æˆåŠŸåˆ›å»ºç»éªŒå›æ”¾ç¼“å†²åŒº")
            return buffer
        except Exception as e:
            print(f"åˆ›å»ºç¼“å†²åŒºæ—¶å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            
            # å°è¯•ä½¿ç”¨æ›´ç®€å•çš„å‚æ•°åˆ›å»ºç¼“å†²åŒº
            try:
                print("å°è¯•ä½¿ç”¨ç®€åŒ–å‚æ•°åˆ›å»ºç¼“å†²åŒº...")
                # æ£€æŸ¥OffPolicyBufferEPçš„æ„é€ å‡½æ•°ç­¾å
                import inspect
                if hasattr(OffPolicyBufferEP, '__init__'):
                    sig = inspect.signature(OffPolicyBufferEP.__init__)
                    print(f"OffPolicyBufferEP.__init__çš„å‚æ•°ç­¾å: {sig}")
                
                # å°è¯•ä½¿ç”¨æœ€å°‘çš„å¿…è¦å‚æ•°
        buffer = OffPolicyBufferEP(
                    buffer_args, share_obs_space, self.num_agents, obs_spaces, act_spaces
        )
                print("âœ“ ä½¿ç”¨ç®€åŒ–å‚æ•°æˆåŠŸåˆ›å»ºç»éªŒå›æ”¾ç¼“å†²åŒº")
        return buffer
            except Exception as e2:
                print(f"ä½¿ç”¨ç®€åŒ–å‚æ•°åˆ›å»ºç¼“å†²åŒºä¹Ÿå¤±è´¥: {e2}")
                raise RuntimeError("æ— æ³•åˆ›å»ºç»éªŒå›æ”¾ç¼“å†²åŒºï¼Œè¯·æ£€æŸ¥OffPolicyBufferEPç±»çš„å®ç°")
    
    def run_validation(self):
        """è¿è¡ŒéªŒè¯è¿‡ç¨‹"""
        print("\n" + "="*60)
        print(f"å¼€å§‹åŸºäºHARLæ¡†æ¶çš„åˆ›æ–°ç‚¹ä¸€éªŒè¯ - {self.mode.upper()}æ¨¡å¼ ({self.ablation_mode})")
        print("éªŒè¯æ–¹æ³•ï¼šç‹¬ç«‹è®­ç»ƒï¼Œç¦»çº¿æ¯”è¾ƒ - ä¸¥æ ¼æ§åˆ¶å˜é‡æ³•")
        print("="*60)
        
        total_steps = 0
        best_reward = -np.inf
        
        for episode in range(self.config['max_episodes']):
            states = self.env.reset()
            for agent in self.agents:
                if hasattr(agent, 'reset_history'):
                agent.reset_history()
            
            episode_reward, episode_length = 0, 0
            episode_transformer_metrics, episode_contrastive_losses = [], []
            episode_attention_weights, episode_state_embeddings = [], []
            
            obs = self._format_observations(states)
            
            for step in range(self.config['max_steps']):
                actions, contrastive_infos = [], []
                attention_weights, state_embeddings = [], []
                
                for i, agent in enumerate(self.agents):
                    # æ£€æŸ¥æ™ºèƒ½ä½“æ˜¯å¦æœ‰use_transformerå±æ€§
                    use_transformer = hasattr(agent, 'use_transformer') and agent.use_transformer
                    use_contrastive = hasattr(agent, 'use_contrastive_learning') and agent.use_contrastive_learning
                    
                    if use_transformer:
                        # ä¿®æ”¹è§£åŒ…æ–¹å¼ï¼Œé€‚åº”å®é™…è¿”å›å€¼æ•°é‡
                        action_result = agent.get_actions_with_logprobs(
                            obs[i], stochastic=True, agent_id=i
                        )
                        
                        # ç¡®ä¿action_resultæ˜¯å…ƒç»„æˆ–åˆ—è¡¨
                        if not isinstance(action_result, (tuple, list)):
                            action_result = (action_result,)
                            
                        # æå–åŠ¨ä½œ
                        action = action_result[0]
                        
                        # åˆå§‹åŒ–å¯é€‰è¿”å›å€¼
                        attn, s_emb, c_info = None, None, None
                        
                        # æ ¹æ®è¿”å›å€¼é•¿åº¦åˆ†é…
                        if len(action_result) >= 3:
                            # å¯èƒ½çš„è¿”å›æ ¼å¼: (action, logprob, attention)
                            attn = action_result[2]
                        
                        # å¦‚æœæœ‰é¢å¤–ä¿¡æ¯ï¼Œå‡è®¾å®ƒåœ¨æœ€åä¸€ä¸ªä½ç½®
                        if len(action_result) >= 4:
                            s_emb = action_result[3]
                        
                        if len(action_result) >= 5:
                            c_info = action_result[4]
                        
                        if self.config['save_attention_weights'] and attn is not None:
                            # æ£€æŸ¥attnçš„ç±»å‹ï¼Œå¦‚æœæ˜¯å­—å…¸åˆ™æå–å…¶ä¸­çš„å¼ é‡ï¼Œå¦‚æœæ˜¯å¼ é‡åˆ™ç›´æ¥ä½¿ç”¨
                            if isinstance(attn, dict):
                                # å°è¯•ä»å­—å…¸ä¸­æå–æ³¨æ„åŠ›æƒé‡
                                for key, value in attn.items():
                                    if isinstance(value, torch.Tensor):
                                        attention_weights.append(value.detach())
                                        break
                            elif isinstance(attn, torch.Tensor):
                            attention_weights.append(attn.detach())
                            
                        if self.config['save_state_embeddings'] and s_emb is not None:
                            # åŒæ ·æ£€æŸ¥s_embçš„ç±»å‹
                            if isinstance(s_emb, dict):
                                for key, value in s_emb.items():
                                    if isinstance(value, torch.Tensor):
                                        state_embeddings.append(value.detach())
                                        break
                            elif isinstance(s_emb, torch.Tensor):
                            state_embeddings.append(s_emb.detach())
                                
                        if use_contrastive and c_info is not None:
                            c_info['states_info'] = torch.tensor(obs[i], device=self.device).unsqueeze(0)
                            contrastive_infos.append(c_info)
                    else:
                        action, _ = agent.get_actions_with_logprobs(obs[i], stochastic=True)
                    actions.append(action)
                
                action_dict = {f'vehicle_{i}': act.cpu().numpy() for i, act in enumerate(actions)}
                next_states, rewards, dones, info = self.env.step(action_dict)
                
                next_obs = self._format_observations(next_states)
                reward_array = np.array([rewards[f'vehicle_{i}'] for i in range(self.num_agents)])
                done_array = np.array([dones[f'vehicle_{i}'] for i in range(self.num_agents)])

                # ä¿®æ­£æ•°æ®å½¢çŠ¶ä»¥åŒ¹é…buffer.insertçš„æœŸæœ›
                # buffer.insertæœŸæœ›çš„æ ¼å¼:
                # obs: [agent_1_obs, agent_2_obs, ...] å…¶ä¸­æ¯ä¸ªagent_i_obså½¢çŠ¶ä¸º(n_rollout_threads, obs_dim)
                # actions: [agent_1_actions, agent_2_actions, ...] å…¶ä¸­æ¯ä¸ªagent_i_actionså½¢çŠ¶ä¸º(n_rollout_threads, act_dim)
                
                # é¦–å…ˆå°†è§‚å¯Ÿå’ŒåŠ¨ä½œæ•°æ®å‡†å¤‡ä¸ºæ­£ç¡®çš„æ ¼å¼
                obs_list = []
                next_obs_list = []
                actions_list = []
                
                for i in range(self.num_agents):
                    # æ¯ä¸ªæ™ºèƒ½ä½“çš„è§‚å¯Ÿå½¢çŠ¶åº”ä¸º(1, obs_dim)
                    obs_list.append(np.array([obs[i]]))
                    next_obs_list.append(np.array([next_obs[i]]))
                    # æ¯ä¸ªæ™ºèƒ½ä½“çš„åŠ¨ä½œå½¢çŠ¶åº”ä¸º(1, act_dim)
                    actions_list.append(np.array([actions[i].cpu().numpy()]))
                
                # å…±äº«è§‚å¯Ÿå½¢çŠ¶åº”ä¸º(1, share_obs_dim)
                share_obs_np = np.concatenate(obs).reshape(1, -1)
                next_share_obs_np = np.concatenate(next_obs).reshape(1, -1)
                
                # è®¡ç®—å¹³å‡å¥–åŠ±ä½œä¸ºç¯å¢ƒå¥–åŠ±
                mean_reward = np.mean(reward_array)
                rewards_np = np.array([[mean_reward]], dtype=np.float32)  # å½¢çŠ¶ä¸º(1, 1)
                
                # å¦‚æœä»»ä½•æ™ºèƒ½ä½“å®Œæˆï¼Œåˆ™ç¯å¢ƒå®Œæˆ
                env_done = dones.get('__all__', False)
                dones_np = np.array([[env_done]], dtype=np.bool_)  # å½¢çŠ¶ä¸º(1, 1)
                dones_env_np = np.array([[env_done]])
                
                # æ´»åŠ¨æ©ç ï¼šå¦‚æœæ™ºèƒ½ä½“æœªå®Œæˆåˆ™ä¸º1ï¼Œå¦åˆ™ä¸º0
                valid_transitions = []
                for i in range(self.num_agents):
                    # æ¯ä¸ªæ™ºèƒ½ä½“çš„æœ‰æ•ˆè½¬æ¢å½¢çŠ¶åº”ä¸º(1, 1)
                    is_valid = 0.0 if dones[f'vehicle_{i}'] else 1.0
                    valid_transitions.append(np.array([[is_valid]], dtype=np.float32))

                # å¯ç”¨åŠ¨ä½œä¸ºNoneï¼Œå› ä¸ºæˆ‘ä»¬ä½¿ç”¨è¿ç»­åŠ¨ä½œç©ºé—´
                available_actions = None
                next_available_actions = None

                # æ‰“å°æ’å…¥æ•°æ®çš„å½¢çŠ¶ä¿¡æ¯ï¼Œç”¨äºè°ƒè¯•
                if step % 50 == 0:
                    print("\næ’å…¥ç¼“å†²åŒºçš„æ•°æ®å½¢çŠ¶:")
                    print(f"share_obs_np: {share_obs_np.shape}")
                    print(f"obs_list[0]: {obs_list[0].shape if obs_list else 'N/A'}")
                    print(f"actions_list[0]: {actions_list[0].shape if actions_list else 'N/A'}")
                    print(f"rewards_np: {rewards_np.shape}")
                    print(f"dones_np: {dones_np.shape}")
                    print(f"valid_transitions[0]: {valid_transitions[0].shape if valid_transitions else 'N/A'}")
                    print(f"dones_env_np: {dones_env_np.shape}")
                    print(f"next_share_obs_np: {next_share_obs_np.shape}")
                    print(f"next_obs_list[0]: {next_obs_list[0].shape if next_obs_list else 'N/A'}")
                
                # å°è¯•æ’å…¥æ•°æ®åˆ°ç¼“å†²åŒº
                try:
                    # ä½¿ç”¨æ ‡å‡†çš„insertæ–¹æ³•
                    self.buffer.insert((
                        share_obs_np, obs_list, actions_list, available_actions, 
                        rewards_np, dones_np, valid_transitions, dones_env_np, 
                        next_share_obs_np, next_obs_list, next_available_actions
                    ))
                except Exception as e:
                    print(f"æ’å…¥æ•°æ®åˆ°ç¼“å†²åŒºæ—¶å‡ºé”™: {e}")
                    import traceback
                    traceback.print_exc()
                    
                    # å°è¯•ä½¿ç”¨æ›´ç®€åŒ–çš„æ–¹å¼æ’å…¥æ•°æ®
                    try:
                        print("å°è¯•ä½¿ç”¨ç®€åŒ–å‚æ•°æ’å…¥æ•°æ®...")
                        self.buffer.insert((
                            share_obs_np, obs_list, actions_list, None, 
                            rewards_np, dones_np, valid_transitions, 
                            dones_env_np, next_share_obs_np, next_obs_list, None
                        ))
                    except Exception as e2:
                        print(f"å†æ¬¡å°è¯•æ’å…¥æ•°æ®å¤±è´¥: {e2}")
                        traceback.print_exc()
                
                # ä½¿ç”¨å±€éƒ¨å˜é‡æ£€æŸ¥æ˜¯å¦æœ‰æ™ºèƒ½ä½“å¯ç”¨äº†transformeræˆ–contrastive learning
                has_transformer = any(hasattr(a, 'use_transformer') and a.use_transformer for a in self.agents)
                has_contrastive = any(hasattr(a, 'use_contrastive_learning') and a.use_contrastive_learning for a in self.agents)
                
                if step % 10 == 0 and has_transformer:
                    episode_transformer_metrics.append(self._evaluate_transformer_effectiveness(info))
                if has_contrastive and contrastive_infos:
                    episode_contrastive_losses.append(self._compute_contrastive_loss(contrastive_infos))
                
                if total_steps > self.config['start_steps'] and total_steps % self.config['update_every'] == 0:
                    self._update_agents()
                
                if attention_weights and episode % self.config['visualization_interval'] == 0:
                    episode_attention_weights.append(attention_weights)
                if state_embeddings and episode % self.config['visualization_interval'] == 0:
                    episode_state_embeddings.append(state_embeddings)
                
                obs = next_obs
                episode_reward += np.mean(reward_array)
                episode_length += 1
                total_steps += 1
                if dones.get('__all__', False):
                    break
            
            self.metrics['episode_rewards'].append(episode_reward)
            self.metrics['episode_lengths'].append(episode_length)
            if episode_transformer_metrics: self.metrics['transformer_effectiveness'].append(np.mean(episode_transformer_metrics))
            if episode_contrastive_losses: self.metrics['contrastive_loss_values'].append(np.mean(episode_contrastive_losses))
            if episode_attention_weights: self.metrics['attention_weights'].append(episode_attention_weights)
            if episode_state_embeddings: self.metrics['state_embeddings'].append(episode_state_embeddings)
            
            if episode % 10 == 0:
                print(f"\nEpisode {episode}: Reward: {episode_reward:.4f}, Length: {episode_length}")
            if episode % 10 == 0:
                self._log_to_tensorboard(episode)
            if episode_reward > best_reward:
                best_reward = episode_reward
                self._save_best_model()
        
        self._generate_final_report()
        if self.config['save_attention_weights'] or self.config['save_state_embeddings']:
            self._generate_qualitative_analysis()
        
        return {'log_dir': self.log_dir, 'best_reward': best_reward, 'mode': self.mode, 'ablation_mode': self.ablation_mode}
    
    def _format_observations(self, states):
        return [states[f'vehicle_{i}'] for i in range(self.num_agents)]
    
    def _evaluate_transformer_effectiveness(self, info):
        """è¯„ä¼°Transformerçš„æœ‰æ•ˆæ€§"""
        # å¦‚æœinfoä¸­æ²¡æœ‰transformer_representationsï¼Œè¿”å›0
        if not info or 'transformer_representations' not in info:
            return 0.0
            
        transformer_reps = info.get('transformer_representations', {})
        
        # å¦‚æœtransformer_repsä¸æ˜¯å­—å…¸ï¼Œå°è¯•è½¬æ¢æˆ–è¿”å›é»˜è®¤å€¼
        if not isinstance(transformer_reps, dict):
            try:
                if isinstance(transformer_reps, (list, np.ndarray)):
                    # å¦‚æœæ˜¯åˆ—è¡¨æˆ–æ•°ç»„ï¼Œè®¡ç®—å…¶ç»Ÿè®¡ç‰¹æ€§
                    transformer_reps = np.array(transformer_reps)
                    diversity = np.std(transformer_reps)
                    richness = np.mean(np.abs(transformer_reps))
                    return diversity * richness
                else:
                    return 0.0
            except Exception:
                return 0.0
        
        quality_scores = []
        for vehicle_id in transformer_reps.keys():
            rep = transformer_reps[vehicle_id]
            # ç¡®ä¿repæ˜¯æ•°å€¼æ•°ç»„
            try:
                rep_array = np.array(rep)
                diversity = np.std(rep_array)
                richness = np.mean(np.abs(rep_array))
            quality_scores.append(diversity * richness)
            except Exception:
                # å¦‚æœæ— æ³•è½¬æ¢ä¸ºæ•°ç»„æˆ–è®¡ç®—ç»Ÿè®¡é‡ï¼Œè·³è¿‡æ­¤è¡¨ç¤º
                continue
                
        return np.mean(quality_scores) if quality_scores else 0.0
    
    def _compute_contrastive_loss(self, contrastive_infos):
        """è®¡ç®—å¯¹æ¯”å­¦ä¹ æŸå¤±"""
        if not contrastive_infos: 
            return 0.0
            
        total_loss, count = 0.0, 0
        
        try:
        for i, agent in enumerate(self.agents):
                # æ£€æŸ¥agentæ˜¯å¦æœ‰use_contrastive_learningå±æ€§
                if not hasattr(agent, 'use_contrastive_learning') or not agent.use_contrastive_learning:
                    continue
                    
                if i >= len(contrastive_infos):
                    continue
                    
                # æ£€æŸ¥agentæ˜¯å¦æœ‰compute_contrastive_lossæ–¹æ³•
                if not hasattr(agent, 'compute_contrastive_loss'):
                    continue
                    
                try:
                    # å°è¯•è®¡ç®—å¯¹æ¯”å­¦ä¹ æŸå¤±
                loss = agent.compute_contrastive_loss(contrastive_infos[i])
                    
                    # ç¡®ä¿lossæ˜¯æ ‡é‡
                    if isinstance(loss, torch.Tensor):
                        loss_value = loss.item()
                        total_loss += loss_value
                count += 1
                except Exception as e:
                    print(f"è®¡ç®—æ™ºèƒ½ä½“{i}çš„å¯¹æ¯”å­¦ä¹ æŸå¤±æ—¶å‡ºé”™: {e}")
                    continue
        except Exception as e:
            print(f"è®¡ç®—å¯¹æ¯”å­¦ä¹ æŸå¤±æ—¶å‡ºé”™: {e}")
            return 0.0
            
        return total_loss / count if count > 0 else 0.0
    
    def _update_agents(self):
        """æ›´æ–°æ™ºèƒ½ä½“ï¼ˆHASACåºè´¯æ›´æ–°æœºåˆ¶ï¼‰"""
        # æ£€æŸ¥ç¼“å†²åŒºæ˜¯å¦æœ‰è¶³å¤Ÿçš„æ•°æ®
        if self.buffer.cur_size < self.config['batch_size']:
            return
        
        # åˆå§‹åŒ–å˜é‡ï¼Œç¡®ä¿å³ä½¿å‡ºé”™ä¹Ÿèƒ½ä½¿ç”¨
        batch_size = self.config['batch_size']
        share_obs_np = np.zeros((batch_size, self.obs_dim_single * self.num_agents))
        next_share_obs_np = np.zeros((batch_size, self.obs_dim_single * self.num_agents))
        
        # åˆå§‹åŒ–åŠ¨ä½œåˆ—è¡¨
        actions_list_np = []
        for i in range(self.num_agents):
            actions_list_np.append(np.zeros((batch_size, self.action_dim_single)))
        actions_np_array = np.array(actions_list_np)
        
        # åˆå§‹åŒ–ä¸‹ä¸€æ­¥åŠ¨ä½œå’Œå¯¹æ•°æ¦‚ç‡
        next_actions_list = []
        next_logp_actions_list = []
        for i in range(self.num_agents):
            next_actions_list.append(np.zeros((batch_size, self.action_dim_single)))
            next_logp_actions_list.append(np.zeros((batch_size,)))
        next_actions_np_array = np.array(next_actions_list)
        next_logp_actions_np_array = np.array(next_logp_actions_list)
        
        obs_batch = None
        share_obs_batch = None
        actions_batch = None
        next_obs_batch = None
        reward_batch = None
        done_batch = None
        
        try:
            # å°è¯•ä»ç¼“å†²åŒºé‡‡æ ·
            sample_data = self.buffer.sample()
            
            if not isinstance(sample_data, tuple) or len(sample_data) < 4:
                print(f"è­¦å‘Š: buffer.sample()è¿”å›çš„æ•°æ®æ ¼å¼ä¸æ­£ç¡®: {type(sample_data)}")
                return
                
            # æå–æ•°æ®
            try:
                share_obs_batch = sample_data[0]  # å…±äº«è§‚å¯Ÿ
                obs_batch = sample_data[1]        # è§‚å¯Ÿ
                actions_batch = sample_data[2]    # åŠ¨ä½œ
                reward_batch = sample_data[4] if len(sample_data) >= 12 else sample_data[3]  # å¥–åŠ±
                done_batch = sample_data[5] if len(sample_data) >= 12 else sample_data[4]    # å®Œæˆæ ‡å¿—
                next_share_obs_batch = sample_data[8] if len(sample_data) >= 12 else sample_data[7]  # ä¸‹ä¸€æ­¥å…±äº«è§‚å¯Ÿ
                next_obs_batch = sample_data[9] if len(sample_data) >= 12 else sample_data[8]        # ä¸‹ä¸€æ­¥è§‚å¯Ÿ
            except Exception as e:
                print(f"ä»sample_dataæå–æ•°æ®æ—¶å‡ºé”™: {e}")
                return
                
            # æ£€æŸ¥æ•°æ®æœ‰æ•ˆæ€§
            if not isinstance(share_obs_batch, torch.Tensor) or not isinstance(obs_batch, torch.Tensor):
                print("è­¦å‘Š: é‡‡æ ·çš„è§‚å¯Ÿæ•°æ®ä¸æ˜¯å¼ é‡")
                # å°è¯•å°†æ•°æ®è½¬æ¢ä¸ºå¼ é‡
                try:
                    if not isinstance(share_obs_batch, torch.Tensor):
                        share_obs_batch = torch.tensor(share_obs_batch, device=self.device, dtype=torch.float32)
                    if not isinstance(obs_batch, torch.Tensor):
                        obs_batch = torch.tensor(obs_batch, device=self.device, dtype=torch.float32)
                    if not isinstance(actions_batch, torch.Tensor):
                        actions_batch = torch.tensor(actions_batch, device=self.device, dtype=torch.float32)
                    if not isinstance(reward_batch, torch.Tensor) and reward_batch is not None:
                        reward_batch = torch.tensor(reward_batch, device=self.device, dtype=torch.float32)
                    if not isinstance(done_batch, torch.Tensor) and done_batch is not None:
                        done_batch = torch.tensor(done_batch, device=self.device, dtype=torch.float32)
                    if not isinstance(next_share_obs_batch, torch.Tensor):
                        next_share_obs_batch = torch.tensor(next_share_obs_batch, device=self.device, dtype=torch.float32)
                    if not isinstance(next_obs_batch, torch.Tensor):
                        next_obs_batch = torch.tensor(next_obs_batch, device=self.device, dtype=torch.float32)
                    print("æˆåŠŸå°†æ•°æ®è½¬æ¢ä¸ºå¼ é‡")
                except Exception as e:
                    print(f"è½¬æ¢æ•°æ®ä¸ºå¼ é‡æ—¶å‡ºé”™: {e}")
                    return
                    
            # æ£€æŸ¥å¹¶ä¿®å¤å¼ é‡ç»´åº¦
            # ç¡®ä¿obs_batchå’Œactions_batchçš„ç»´åº¦æ­£ç¡®
            # é¢„æœŸå½¢çŠ¶: obs_batch [batch_size, n_agents, obs_dim]
            #          actions_batch [batch_size, n_agents, act_dim]
            
            # æ£€æŸ¥obs_batchçš„ç»´åº¦
            if len(obs_batch.shape) == 2:  # [batch_size, obs_dim]
                # æ‰©å±•ä¸º [batch_size, n_agents, obs_dim]
                obs_batch = obs_batch.unsqueeze(1).expand(-1, self.num_agents, -1)
                print(f"æ‰©å±•obs_batchç»´åº¦: {obs_batch.shape}")
            
            # æ£€æŸ¥actions_batchçš„ç»´åº¦
            if len(actions_batch.shape) == 2:  # [batch_size, act_dim]
                # æ‰©å±•ä¸º [batch_size, n_agents, act_dim]
                actions_batch = actions_batch.unsqueeze(1).expand(-1, self.num_agents, -1)
                print(f"æ‰©å±•actions_batchç»´åº¦: {actions_batch.shape}")
            
            # æ£€æŸ¥next_obs_batchçš„ç»´åº¦
            if len(next_obs_batch.shape) == 2:  # [batch_size, obs_dim]
                # æ‰©å±•ä¸º [batch_size, n_agents, obs_dim]
                next_obs_batch = next_obs_batch.unsqueeze(1).expand(-1, self.num_agents, -1)
                print(f"æ‰©å±•next_obs_batchç»´åº¦: {next_obs_batch.shape}")
                
            # è½¬æ¢ä¸ºnumpyæ•°ç»„
            try:
        share_obs_np = share_obs_batch.cpu().numpy()
        next_share_obs_np = next_share_obs_batch.cpu().numpy()
                
                # ç¡®ä¿actions_list_npæ˜¯ä¸€ä¸ªnumpyæ•°ç»„ï¼Œè€Œä¸æ˜¯åˆ—è¡¨
                actions_np = actions_batch.cpu().numpy()  # å½¢çŠ¶åº”ä¸º [batch_size, n_agents, act_dim]
                
                # åˆ›å»ºæ¯ä¸ªæ™ºèƒ½ä½“çš„åŠ¨ä½œæ•°ç»„åˆ—è¡¨
                actions_list_np = []
                for i in range(self.num_agents):
                    agent_actions = actions_np[:, i] if len(actions_np.shape) > 2 else actions_np
                    actions_list_np.append(agent_actions)
                
                # æ›´æ–°batch_sizeä»¥åŒ¹é…å®é™…æ•°æ®
                if share_obs_np.shape[0] != batch_size:
                    print(f"è­¦å‘Š: share_obs_npçš„batch_sizeä¸åŒ¹é…: {share_obs_np.shape[0]} vs {batch_size}")
                    batch_size = share_obs_np.shape[0]
                
            except Exception as e:
                print(f"è½¬æ¢å¼ é‡åˆ°numpyæ•°ç»„æ—¶å‡ºé”™: {e}")
                return
        except Exception as e:
            print(f"é‡‡æ ·æˆ–æ•°æ®å¤„ç†å‡ºé”™: {e}")
            return
            
        try:
        next_actions_list, next_logp_actions_list = [], []
        with torch.no_grad():
            for i in range(self.num_agents):
                agent, agent_next_obs = self.agents[i], next_obs_batch[:, i]
                    
                    try:
                        if hasattr(agent, 'use_transformer') and agent.use_transformer:
                            # ä½¿ç”¨æ›´å¥å£®çš„æ–¹å¼å¤„ç†è¿”å›å€¼
                            action_result = agent.get_actions_with_logprobs(agent_next_obs, stochastic=True, agent_id=i)
                            
                            # ç¡®ä¿action_resultæ˜¯å…ƒç»„æˆ–åˆ—è¡¨
                            if not isinstance(action_result, (tuple, list)):
                                action_result = (action_result,)
                                
                            # æå–åŠ¨ä½œå’Œå¯¹æ•°æ¦‚ç‡
                            next_action = action_result[0]
                            next_logp = torch.zeros(next_action.shape[0], device=self.device) if len(action_result) < 2 else action_result[1]
                else:
                    next_action, next_logp = agent.get_actions_with_logprobs(agent_next_obs, stochastic=True)
                            
                        # ç¡®ä¿next_actionå’Œnext_logpæ˜¯å¼ é‡
                        if not isinstance(next_action, torch.Tensor):
                            next_action = torch.tensor(next_action, device=self.device)
                        if not isinstance(next_logp, torch.Tensor):
                            next_logp = torch.tensor(next_logp, device=self.device)
                            
                        next_actions_list.append(next_action)
                        next_logp_actions_list.append(next_logp)
                    except Exception as e:
                        print(f"å¤„ç†æ™ºèƒ½ä½“{i}çš„ä¸‹ä¸€æ­¥åŠ¨ä½œæ—¶å‡ºé”™: {e}")
                        # åˆ›å»ºé›¶æ•°ç»„ä½œä¸ºé»˜è®¤å€¼
                        batch_size = agent_next_obs.shape[0]
                        action_dim = self.action_dim_single
                        next_actions_list.append(torch.zeros((batch_size, action_dim), device=self.device))
                        next_logp_actions_list.append(torch.zeros(batch_size, device=self.device))
            
            # åˆ›å»ºæœ‰æ•ˆè½¬æ¢å¼ é‡åˆ—è¡¨
            valid_transition_list = [torch.ones((batch_size, 1), device=self.device) for _ in range(self.num_agents)]
            
            # åˆ›å»ºgammaå¼ é‡
            gamma_tensor = torch.full((batch_size, 1), self.config['gamma'], device=self.device)

            # è½¬æ¢æ‰€æœ‰è¾“å…¥ä¸ºå¼ é‡
            share_obs_tensor = torch.tensor(share_obs_np, device=self.device)
            
            # è½¬æ¢actions_list_npä¸ºå¼ é‡åˆ—è¡¨
            actions_tensor_list = []
            for agent_actions in actions_list_np:
                actions_tensor_list.append(torch.tensor(agent_actions, device=self.device))
            
            reward_tensor = reward_batch if isinstance(reward_batch, torch.Tensor) else torch.tensor(reward_batch, device=self.device)
            done_tensor = done_batch if isinstance(done_batch, torch.Tensor) else torch.tensor(done_batch, device=self.device)
            next_share_obs_tensor = torch.tensor(next_share_obs_np, device=self.device)

        agent_indices = list(range(self.num_agents))
        np.random.shuffle(agent_indices)
        
        for agent_idx in agent_indices:
            agent, critic = self.agents[agent_idx], self.critics[agent_idx]
                
                try:
                    # æ‰“å°æ¯ä¸ªå‚æ•°çš„å½¢çŠ¶ï¼Œç”¨äºè°ƒè¯•
                    print(f"æ™ºèƒ½ä½“{agent_idx}çš„è®­ç»ƒå‚æ•°å½¢çŠ¶:")
                    print(f"share_obs_tensor: {share_obs_tensor.shape}")
                    print(f"actions_tensor_list[0]: {actions_tensor_list[0].shape if actions_tensor_list else 'N/A'}")
                    print(f"reward_tensor: {reward_tensor.shape}")
                    print(f"done_tensor: {done_tensor.shape}")
                    print(f"valid_transition_list[0]: {valid_transition_list[0].shape}")
                    print(f"next_share_obs_tensor: {next_share_obs_tensor.shape}")
                    print(f"next_actions_list[0]: {next_actions_list[0].shape if next_actions_list else 'N/A'}")
                    print(f"next_logp_actions_list[0]: {next_logp_actions_list[0].shape if next_logp_actions_list else 'N/A'}")
                    print(f"gamma_tensor: {gamma_tensor.shape}")
                    
                    # è°ƒç”¨critic.trainæ–¹æ³•
            critic.train(
                        share_obs_tensor,
                        actions_tensor_list,
                        reward_tensor,
                        done_tensor,
                        valid_transition_list,
                        done_tensor,  # ä½¿ç”¨done_tensorä½œä¸ºtermå‚æ•°
                        next_share_obs_tensor,
                        next_actions_list,
                        next_logp_actions_list,
                        gamma_tensor
                    )
                    
                    # æ›´æ–°actor
            self._update_actor(agent, critic, (obs_batch, share_obs_batch, actions_batch), agent_idx)
                    
                    # è½¯æ›´æ–°
                    if hasattr(critic, 'soft_update') and callable(critic.soft_update):
            critic.soft_update()
                    else:
                        print(f"è­¦å‘Š: æ™ºèƒ½ä½“{agent_idx}çš„criticæ²¡æœ‰soft_updateæ–¹æ³•")
                except Exception as e:
                    print(f"è®­ç»ƒæ™ºèƒ½ä½“{agent_idx}æ—¶å‡ºé”™: {e}")
                    import traceback
                    traceback.print_exc()
                    continue
        except Exception as e:
            print(f"æ›´æ–°æ™ºèƒ½ä½“æ—¶å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            return

    def _update_actor(self, agent, critic, sample, agent_idx):
        """æ›´æ–°Actorï¼ˆåŸºäºCTDEèŒƒå¼ï¼‰"""
        obs_batch, share_obs_batch, actions_batch = sample
        
        try:
            # æ£€æŸ¥obs_batchçš„ç»´åº¦æ˜¯å¦æ­£ç¡®
            if obs_batch is None:
                print(f"è­¦å‘Š: obs_batchä¸ºNoneï¼Œè·³è¿‡æ›´æ–°æ™ºèƒ½ä½“{agent_idx}")
                return 0.0
                
            # è·å–å½“å‰æ™ºèƒ½ä½“çš„è§‚å¯Ÿ
            try:
        obs = obs_batch[:, agent_idx]
            except IndexError as e:
                print(f"è·å–æ™ºèƒ½ä½“{agent_idx}çš„è§‚å¯Ÿæ—¶å‡ºé”™: {e}")
                print(f"obs_batchå½¢çŠ¶: {obs_batch.shape}")
                # å¦‚æœç´¢å¼•è¶…å‡ºèŒƒå›´ï¼Œå¯èƒ½æ˜¯ç»´åº¦ä¸æ­£ç¡®ï¼Œå°è¯•ä½¿ç”¨ç¬¬ä¸€ä¸ªæ™ºèƒ½ä½“çš„æ•°æ®
                if obs_batch.shape[1] > 0:
                    obs = obs_batch[:, 0]
                    print(f"ä½¿ç”¨æ™ºèƒ½ä½“0çš„è§‚å¯Ÿæ•°æ®ä»£æ›¿æ™ºèƒ½ä½“{agent_idx}")
                else:
                    print(f"æ— æ³•è·å–ä»»ä½•æ™ºèƒ½ä½“çš„è§‚å¯Ÿæ•°æ®ï¼Œè·³è¿‡æ›´æ–°")
                    return 0.0
        
        contrastive_info = None
        if hasattr(agent, 'use_transformer') and agent.use_transformer:
            try:
                # ä¿®æ”¹è§£åŒ…æ–¹å¼ï¼Œé€‚åº”å®é™…è¿”å›å€¼æ•°é‡
                action_result = agent.get_actions_with_logprobs(
                obs, stochastic=True, agent_id=agent_idx
            )
                
                # ç¡®ä¿action_resultæ˜¯å…ƒç»„æˆ–åˆ—è¡¨
                if not isinstance(action_result, (tuple, list)):
                    action_result = (action_result,)
                
                # æå–åŠ¨ä½œå’Œå¯¹æ•°æ¦‚ç‡
                new_actions = action_result[0]
                log_probs = torch.zeros(new_actions.shape[0], device=self.device) if len(action_result) < 2 else action_result[1]
                
                # å¦‚æœæœ‰é¢å¤–è¿”å›å€¼ï¼Œå‡è®¾æœ€åä¸€ä¸ªæ˜¯contrastive_info
                if len(action_result) >= 5:
                    contrastive_info = action_result[4]
                elif len(action_result) >= 3:
                    # å°è¯•ä»ç¬¬ä¸‰ä¸ªä½ç½®è·å–contrastive_infoï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                    contrastive_info = action_result[2]
            except Exception as e:
                print(f"è·å–æ™ºèƒ½ä½“{agent_idx}çš„åŠ¨ä½œæ—¶å‡ºé”™: {e}")
                import traceback
                traceback.print_exc()
                # åˆ›å»ºé»˜è®¤åŠ¨ä½œå’Œå¯¹æ•°æ¦‚ç‡
                new_actions = torch.zeros((obs.shape[0], self.action_dim_single), device=self.device)
                log_probs = torch.zeros(obs.shape[0], device=self.device)
        else:
            try:
            new_actions, log_probs = agent.get_actions_with_logprobs(obs, stochastic=True)
            except Exception as e:
                print(f"è·å–æ™ºèƒ½ä½“{agent_idx}çš„åŠ¨ä½œæ—¶å‡ºé”™: {e}")
                import traceback
                traceback.print_exc()
                # åˆ›å»ºé»˜è®¤åŠ¨ä½œå’Œå¯¹æ•°æ¦‚ç‡
                new_actions = torch.zeros((obs.shape[0], self.action_dim_single), device=self.device)
                log_probs = torch.zeros(obs.shape[0], device=self.device)
            
            # åˆ›å»ºè”åˆåŠ¨ä½œ
            try:
        joint_actions = actions_batch.clone()
        joint_actions[:, agent_idx] = new_actions
            except IndexError as e:
                print(f"æ›´æ–°è”åˆåŠ¨ä½œæ—¶å‡ºé”™: {e}")
                print(f"actions_batchå½¢çŠ¶: {actions_batch.shape}, new_actionså½¢çŠ¶: {new_actions.shape}")
                # å¦‚æœç´¢å¼•è¶…å‡ºèŒƒå›´ï¼Œå¯èƒ½éœ€è¦é‡æ–°åˆ›å»ºjoint_actions
                if len(actions_batch.shape) < 3:
                    # å¦‚æœactions_batchä¸æ˜¯3Dçš„ï¼Œå°è¯•æ‰©å±•å®ƒ
                    joint_actions = actions_batch.unsqueeze(1).expand(-1, self.num_agents, -1).clone()
                    joint_actions[:, agent_idx] = new_actions
                else:
                    # ä½¿ç”¨åŸå§‹actions_batch
                    joint_actions = actions_batch
            
            # è·å–Qå€¼
            try:
        q_values = critic.get_values(share_obs_batch, joint_actions)
            except Exception as e:
                print(f"è·å–æ™ºèƒ½ä½“{agent_idx}çš„Qå€¼æ—¶å‡ºé”™: {e}")
                import traceback
                traceback.print_exc()
                # åˆ›å»ºé»˜è®¤Qå€¼
                q_values = torch.zeros(obs.shape[0], device=self.device)
                
            # è®¡ç®—ç­–ç•¥æŸå¤±
            try:
        policy_loss = -(q_values - self.config['alpha'] * log_probs).mean()
        
                # å¦‚æœå¯ç”¨äº†å¯¹æ¯”å­¦ä¹ ï¼Œæ·»åŠ å¯¹æ¯”æŸå¤±
                if hasattr(agent, 'use_contrastive_learning') and agent.use_contrastive_learning and contrastive_info is not None:
                    try:
            contrastive_info['states_info'] = obs
            contrastive_loss = agent.compute_contrastive_loss(contrastive_info)
            policy_loss += self.config['contrastive_loss_weight'] * contrastive_loss
                    except Exception as e:
                        print(f"è®¡ç®—å¯¹æ¯”æŸå¤±æ—¶å‡ºé”™: {e}")
                        import traceback
                        traceback.print_exc()
        
                # æ‰§è¡Œæ¢¯åº¦æ›´æ–°
                if hasattr(agent, 'actor_optimizer') and agent.actor_optimizer is not None:
        agent.actor_optimizer.zero_grad()
        policy_loss.backward()
        agent.actor_optimizer.step()
                else:
                    print(f"è­¦å‘Š: æ™ºèƒ½ä½“{agent_idx}æ²¡æœ‰actor_optimizerå±æ€§æˆ–ä¸ºNone")
                    
        return policy_loss.item()
            except Exception as e:
                print(f"è®¡ç®—æˆ–ä¼˜åŒ–ç­–ç•¥æŸå¤±æ—¶å‡ºé”™: {e}")
                import traceback
                traceback.print_exc()
                return 0.0
        except Exception as e:
            print(f"æ›´æ–°æ™ºèƒ½ä½“{agent_idx}æ—¶å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            return 0.0
    
    def _log_to_tensorboard(self, episode):
        """è®°å½•åˆ°TensorBoard"""
        if self.metrics['episode_rewards']: self.writer.add_scalar('Episode/Reward', self.metrics['episode_rewards'][-1], episode)
        if self.metrics['episode_lengths']: self.writer.add_scalar('Episode/Length', self.metrics['episode_lengths'][-1], episode)
        if self.metrics['transformer_effectiveness']: self.writer.add_scalar('Innovation1/Transformer_Effectiveness', self.metrics['transformer_effectiveness'][-1], episode)
        if self.metrics['contrastive_loss_values']: self.writer.add_scalar('Innovation1/Contrastive_Loss', self.metrics['contrastive_loss_values'][-1], episode)
    
    def _save_best_model(self):
        """ä¿å­˜æœ€ä½³æ¨¡å‹"""
        model_dir = os.path.join(self.log_dir, "best_model")
        os.makedirs(model_dir, exist_ok=True)
        for i, agent in enumerate(self.agents):
            agent.save(model_dir, i)
    
    def _generate_final_report(self):
        """ç”ŸæˆéªŒè¯æŠ¥å‘Š"""
        avg_reward = np.mean(self.metrics['episode_rewards'][-100:]) if self.metrics['episode_rewards'] else 0
        avg_length = np.mean(self.metrics['episode_lengths'][-100:]) if self.metrics['episode_lengths'] else 0
        transformer_effectiveness = np.mean(self.metrics['transformer_effectiveness']) if self.metrics['transformer_effectiveness'] else 0
        contrastive_loss = np.mean(self.metrics['contrastive_loss_values']) if self.metrics['contrastive_loss_values'] else 0
        
        report = {
            'mode': self.mode, 'ablation_mode': self.ablation_mode, 'timestamp': time.time(),
            'avg_reward': float(avg_reward), 'avg_episode_length': float(avg_length),
            'total_episodes': len(self.metrics['episode_rewards']),
            'transformer_enabled': self.use_transformer_flag,
            'contrastive_learning_enabled': self.use_contrastive_learning_flag,
        }
        if self.use_transformer_flag: report['transformer_effectiveness'] = float(transformer_effectiveness)
        if self.use_contrastive_learning_flag: report['contrastive_loss'] = float(contrastive_loss)
        
        report_path = os.path.join(self.log_dir, 'validation_report.json')
        with open(report_path, 'w') as f: json.dump(report, f, indent=2)
        print(f"\nâœ“ éªŒè¯æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_path}")
        self._create_visualization()

    def _generate_qualitative_analysis(self):
        """ç”Ÿæˆå®šæ€§åˆ†æå¯è§†åŒ–"""
        qualitative_dir = os.path.join(self.log_dir, 'qualitative_analysis')
        os.makedirs(qualitative_dir, exist_ok=True)
        if self.metrics['attention_weights'] and self.config['save_attention_weights']: self._visualize_attention_weights(qualitative_dir)
        if self.metrics['state_embeddings'] and self.config['save_state_embeddings']: self._visualize_state_embeddings(qualitative_dir)
        print(f"âœ“ å®šæ€§åˆ†æå¯è§†åŒ–å·²ä¿å­˜åˆ°: {qualitative_dir}")

    def _visualize_attention_weights(self, save_dir):
        """å¯è§†åŒ–æ³¨æ„åŠ›æƒé‡"""
        if not self.metrics['attention_weights']: return
        attention_data = self.metrics['attention_weights'][-1]
        sample_indices = [0, len(attention_data)//2, -1]
        
        for idx, step_idx in enumerate(sample_indices):
            if step_idx < 0 and abs(step_idx) > len(attention_data): continue
            step_data = attention_data[step_idx]
            for agent_idx, agent_attn in enumerate(step_data):
                if agent_attn.dim() < 3: continue
                for head_idx in range(agent_attn.shape[0]):
                    plt.figure(figsize=(8, 6))
                    plt.imshow(agent_attn[head_idx].cpu().numpy(), cmap='viridis')
                    plt.colorbar()
                    plt.title(f'Agent {agent_idx}, Head {head_idx}, Step {step_idx}')
                    plt.xlabel('Sequence Position'); plt.ylabel('Attention')
                    plt.savefig(os.path.join(save_dir, f'attn_agent{agent_idx}_head{head_idx}_step{step_idx}.png'))
                    plt.close()

    def _visualize_state_embeddings(self, save_dir):
        """ä½¿ç”¨t-SNEå¯è§†åŒ–çŠ¶æ€åµŒå…¥"""
        if not self.metrics['state_embeddings']:
            return
            
        # è·³è¿‡å¯è§†åŒ–ï¼Œé¿å…å¯¼å…¥é”™è¯¯
        # åœ¨å®é™…ç¯å¢ƒä¸­ï¼Œè¯·ç¡®ä¿å®‰è£…äº†sklearnåº“
        print("æ³¨æ„: è·³è¿‡çŠ¶æ€åµŒå…¥å¯è§†åŒ–ï¼Œéœ€è¦å®‰è£…sklearnåº“")
        return
        
        # ä»¥ä¸‹ä»£ç åœ¨å®‰è£…äº†sklearnæ—¶å¯ä»¥å–æ¶ˆæ³¨é‡Š
        """
        # å°è¯•å¯¼å…¥sklearn
        try:
            import sklearn
        from sklearn.manifold import TSNE
        except ImportError:
            print("è­¦å‘Š: æ— æ³•å¯¼å…¥sklearn.manifold.TSNEï¼Œè·³è¿‡çŠ¶æ€åµŒå…¥å¯è§†åŒ–")
            return
        except Exception as e:
            print(f"è­¦å‘Š: å¯¼å…¥sklearnæ—¶å‡ºé”™: {e}ï¼Œè·³è¿‡çŠ¶æ€åµŒå…¥å¯è§†åŒ–")
            return
        
        all_embeddings, all_agent_ids, all_step_ids = [], [], []
        for episode_idx, episode_data in enumerate(self.metrics['state_embeddings']):
            for step_idx, step_data in enumerate(episode_data):
                for agent_idx, agent_emb in enumerate(step_data):
                    all_embeddings.append(agent_emb.cpu().numpy())
                    all_agent_ids.append(agent_idx)
                    all_step_ids.append(step_idx)
        
        if not all_embeddings: 
            return
            
        embeddings_matrix = np.vstack(all_embeddings)
        perplexity_value = min(30, len(embeddings_matrix) - 1)
        if perplexity_value <= 0: 
            return

        tsne = TSNE(n_components=2, random_state=42, perplexity=perplexity_value)
        embeddings_2d = tsne.fit_transform(embeddings_matrix)
        
        plt.figure(figsize=(12, 10))
        for agent_id in np.unique(all_agent_ids):
            agent_mask = np.array(all_agent_ids) == agent_id
            plt.scatter(embeddings_2d[agent_mask, 0], embeddings_2d[agent_mask, 1], label=f'Agent {agent_id}', alpha=0.7)
        plt.legend(); plt.title('t-SNE Visualization of State Embeddings by Agent')
        plt.savefig(os.path.join(save_dir, 'tsne_by_agent.png')); plt.close()
        
        plt.figure(figsize=(12, 10))
        scatter = plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], c=all_step_ids, cmap='viridis', alpha=0.7)
        plt.colorbar(scatter, label='Time Step'); plt.title('t-SNE Visualization of State Embeddings by Time Step')
        plt.savefig(os.path.join(save_dir, 'tsne_by_timestep.png')); plt.close()
        """

    def _create_visualization(self):
        """åˆ›å»ºç»“æœå¯è§†åŒ–"""
        plt.figure(figsize=(10, 6)); plt.plot(self.metrics['episode_rewards'])
        plt.title(f'Episode Rewards - {self.mode.upper()} Mode'); plt.xlabel('Episode'); plt.ylabel('Reward')
        plt.grid(True); plt.savefig(os.path.join(self.log_dir, 'rewards.png')); plt.close()
        
        if self.metrics['transformer_effectiveness'] and self.use_transformer_flag:
            plt.figure(figsize=(10, 6)); plt.plot(self.metrics['transformer_effectiveness'])
            plt.title('Transformer Effectiveness'); plt.xlabel('Evaluation'); plt.ylabel('Effectiveness Score')
            plt.grid(True); plt.savefig(os.path.join(self.log_dir, 'transformer_effectiveness.png')); plt.close()
        
        if self.metrics['contrastive_loss_values'] and self.use_contrastive_learning_flag:
            plt.figure(figsize=(10, 6)); plt.plot(self.metrics['contrastive_loss_values'])
            plt.title('Contrastive Learning Loss'); plt.xlabel('Evaluation'); plt.ylabel('Loss')
            plt.grid(True); plt.savefig(os.path.join(self.log_dir, 'contrastive_loss.png')); plt.close()
        
        print(f"âœ“ å¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜åˆ°: {self.log_dir}")

def main():
    """ä¸»å‡½æ•°"""
    print("åŸºäºHARLæ¡†æ¶çš„åˆ›æ–°ç‚¹ä¸€ç²¾å‡†éªŒè¯ç³»ç»Ÿ")
    print("ä½¿ç”¨çœŸå®çš„HASACç®—æ³•ã€TransformerEnhancedPolicyå’Œå¯¹æ¯”å­¦ä¹ ")
    print("éªŒè¯æ–¹æ³•ï¼šç‹¬ç«‹è®­ç»ƒï¼Œç¦»çº¿æ¯”è¾ƒ - ä¸¥æ ¼æ§åˆ¶å˜é‡æ³•")
    print("="*60)
    
    validator = HARLBasedInnovation1Validator()
    start_time = time.time()
    validation_results = validator.run_validation()
    end_time = time.time()
    
    print(f"\néªŒè¯å®Œæˆï¼æ€»ç”¨æ—¶: {end_time - start_time:.2f} ç§’")
    print(f"éªŒè¯ç»“æœå·²ä¿å­˜è‡³: {validation_results['log_dir']}")
    print(f"å®éªŒæ¨¡å¼: {validation_results['mode']}, æ¶ˆèæ¨¡å¼: {validation_results['ablation_mode']}")
    print("\nè¦æ¯”è¾ƒä¸åŒæ¨¡å¼çš„ç»“æœï¼Œè¯·è¿è¡Œ python compare_results.py")

if __name__ == "__main__":
    main()
