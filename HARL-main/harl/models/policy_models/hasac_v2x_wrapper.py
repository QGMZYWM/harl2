#!/usr/bin/env python3
"""
HASAC V2Xç¯å¢ƒåŒ…è£…å™¨
å°†HASACçš„è¿ç»­åŠ¨ä½œè½¬æ¢ä¸ºV2Xç¯å¢ƒéœ€è¦çš„ç¦»æ•£åŠ¨ä½œ
ä¿æŒåŸV2Xç¯å¢ƒä¸å˜ï¼Œé€šè¿‡åŒ…è£…å™¨å®ç°å…¼å®¹
"""

import numpy as np
import gym
from gym import spaces
import torch
import torch.nn.functional as F

class HASACV2XWrapper(gym.Wrapper):
    """
    HASAC V2Xç¯å¢ƒåŒ…è£…å™¨
    å°†è¿ç»­åŠ¨ä½œç©ºé—´è½¬æ¢ä¸ºç¦»æ•£åŠ¨ä½œç©ºé—´
    """
    
    def __init__(self, env):
        super().__init__(env)
        
        # ä¿å­˜åŸå§‹ç¯å¢ƒçš„ç¦»æ•£åŠ¨ä½œç©ºé—´
        self.original_action_space = env.action_space
        
        # å®‰å…¨åœ°è·å–åŠ¨ä½œç©ºé—´ç»´åº¦
        try:
            # å°è¯•è·å–ç¬¬ä¸€ä¸ªæ™ºèƒ½ä½“çš„åŠ¨ä½œç©ºé—´
            if isinstance(env.action_space, list) and len(env.action_space) > 0:
                first_agent_space = env.action_space[0]
                if hasattr(first_agent_space, 'n'):
                    self.num_targets = first_agent_space.n
                else:
                    # é»˜è®¤å€¼ï¼Œå¦‚æœæ— æ³•è·å–
                    self.num_targets = 5
            else:
                # å¦‚æœaction_spaceä¸æ˜¯åˆ—è¡¨ï¼Œå°è¯•ç›´æ¥è·å–
                if hasattr(env.action_space, 'n'):
                    self.num_targets = env.action_space.n
                else:
                    # é»˜è®¤å€¼
                    self.num_targets = 5
        except:
            # å¦‚æœä¸Šè¿°æ–¹æ³•éƒ½å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼
            self.num_targets = 5
        
        # ä¸ºHASACåˆ›å»ºè¿ç»­åŠ¨ä½œç©ºé—´
        # [ç›®æ ‡é€‰æ‹©æƒé‡(num_targets), ä¼ è¾“åŠŸç‡, ä»»åŠ¡ä¼˜å…ˆçº§, è´Ÿè½½å‡è¡¡å› å­]
        action_dim = self.num_targets + 3
        single_action_space = spaces.Box(
            low=-1.0, high=1.0, 
            shape=(action_dim,), 
            dtype=np.float32
        )
        
        # å¤šæ™ºèƒ½ä½“åŠ¨ä½œç©ºé—´
        self.n_agents = getattr(env, 'n_agents', 1)
        self.action_space = [single_action_space for _ in range(self.n_agents)]
        
        print(f"ğŸ”„ HASAC V2X Wrapper initialized:")
        print(f"   - Original action space: Discrete({self.num_targets})")
        print(f"   - New action space: Box(shape=({action_dim},))")
        print(f"   - Action components: [target_weights({self.num_targets}), power(1), priority(1), balance(1)]")
    
    def step(self, continuous_actions):
        """
        å°†è¿ç»­åŠ¨ä½œè½¬æ¢ä¸ºç¦»æ•£åŠ¨ä½œå¹¶æ‰§è¡Œ
        
        Args:
            continuous_actions: List of continuous actions for each agent
            
        Returns:
            Standard gym environment returns
        """
        # è½¬æ¢æ¯ä¸ªæ™ºèƒ½ä½“çš„è¿ç»­åŠ¨ä½œä¸ºç¦»æ•£åŠ¨ä½œ
        discrete_actions = []
        additional_controls = []
        
        for agent_id, cont_action in enumerate(continuous_actions):
            discrete_action, controls = self._convert_action(agent_id, cont_action)
            discrete_actions.append(discrete_action)
            additional_controls.append(controls)
        
        # åœ¨åŸç¯å¢ƒä¸­æ‰§è¡Œç¦»æ•£åŠ¨ä½œ
        obs, share_obs, rewards, dones, infos, available_actions = self.env.step(discrete_actions)
        
        # ä½¿ç”¨é¢å¤–æ§åˆ¶ä¿¡æ¯è°ƒæ•´å¥–åŠ±
        enhanced_rewards = self._enhance_rewards(rewards, additional_controls, infos)
        
        return obs, share_obs, enhanced_rewards, dones, infos, available_actions
    
    def _convert_action(self, agent_id, continuous_action):
        """
        å°†å•ä¸ªæ™ºèƒ½ä½“çš„è¿ç»­åŠ¨ä½œè½¬æ¢ä¸ºç¦»æ•£åŠ¨ä½œ
        
        Args:
            agent_id: æ™ºèƒ½ä½“ID
            continuous_action: è¿ç»­åŠ¨ä½œ [target_weights, power, priority, balance]
            
        Returns:
            discrete_action: ç¦»æ•£åŠ¨ä½œ
            controls: é¢å¤–æ§åˆ¶ä¿¡æ¯
        """
        # åˆ†è§£è¿ç»­åŠ¨ä½œ
        target_weights = continuous_action[:self.num_targets]
        power_level = continuous_action[self.num_targets]
        priority_weight = continuous_action[self.num_targets + 1] 
        balance_factor = continuous_action[self.num_targets + 2]
        
        # ä½¿ç”¨Gumbel-Softmaxæˆ–ç®€å•çš„argmaxé€‰æ‹©ç›®æ ‡
        if self.training:
            # è®­ç»ƒæ—¶ä½¿ç”¨Gumbel-Softmaxä¿æŒå¯å¾®æ€§
            discrete_action = self._gumbel_softmax_select(target_weights)
        else:
            # æ¨ç†æ—¶ä½¿ç”¨argmax
            discrete_action = np.argmax(target_weights)
        
        # é¢å¤–æ§åˆ¶ä¿¡æ¯
        controls = {
            'power_level': (power_level + 1) / 2,  # è½¬æ¢åˆ°[0, 1]
            'priority_weight': (priority_weight + 1) / 2,  # è½¬æ¢åˆ°[0, 1] 
            'balance_factor': (balance_factor + 1) / 2,  # è½¬æ¢åˆ°[0, 1]
            'target_weights': target_weights
        }
        
        return discrete_action, controls
    
    def _gumbel_softmax_select(self, logits, temperature=1.0):
        """
        ä½¿ç”¨Gumbel-Softmaxè¿›è¡Œå¯å¾®çš„ç¦»æ•£é€‰æ‹©
        
        Args:
            logits: ç›®æ ‡æƒé‡
            temperature: æ¸©åº¦å‚æ•°
            
        Returns:
            é€‰æ‹©çš„ç›®æ ‡ç´¢å¼•
        """
        # æ·»åŠ Gumbelå™ªå£°
        gumbel_noise = -np.log(-np.log(np.random.uniform(0, 1, logits.shape) + 1e-20) + 1e-20)
        y = logits + gumbel_noise
        
        # Softmaxé€‰æ‹©
        probabilities = F.softmax(torch.tensor(y / temperature), dim=0).numpy()
        
        # é‡‡æ ·é€‰æ‹©
        return np.random.choice(len(probabilities), p=probabilities)
    
    def _enhance_rewards(self, original_rewards, controls_list, infos):
        """
        ä½¿ç”¨é¢å¤–æ§åˆ¶ä¿¡æ¯å¢å¼ºå¥–åŠ±
        
        Args:
            original_rewards: åŸå§‹å¥–åŠ±
            controls_list: é¢å¤–æ§åˆ¶ä¿¡æ¯åˆ—è¡¨
            infos: ç¯å¢ƒä¿¡æ¯
            
        Returns:
            å¢å¼ºåçš„å¥–åŠ±
        """
        enhanced_rewards = []
        
        for i, (reward, controls) in enumerate(zip(original_rewards, controls_list)):
            enhanced_reward = reward[0] if isinstance(reward, list) else reward
            
            # åŠŸç‡æ•ˆç‡å¥–åŠ±
            power_efficiency = 1.0 - abs(controls['power_level'] - 0.5)  # ä¸­ç­‰åŠŸç‡æœ€ä¼˜
            enhanced_reward += power_efficiency * 0.5
            
            # è´Ÿè½½å‡è¡¡å¥–åŠ±
            if 'current_load' in infos[i]:
                target_load = 0.5  # ç›®æ ‡è´Ÿè½½50%
                actual_load = infos[i]['current_load']
                balance_bonus = controls['balance_factor'] * (1.0 - abs(actual_load - target_load))
                enhanced_reward += balance_bonus * 0.3
            
            # ä¼˜å…ˆçº§è°ƒåº¦å¥–åŠ±
            if 'completed_tasks' in infos[i] and infos[i]['completed_tasks'] > 0:
                priority_bonus = controls['priority_weight'] * 0.2
                enhanced_reward += priority_bonus
            
            enhanced_rewards.append([enhanced_reward])
        
        return enhanced_rewards
    
    def reset(self):
        """é‡ç½®ç¯å¢ƒ"""
        return self.env.reset()
    
    def render(self, mode='human'):
        """æ¸²æŸ“ç¯å¢ƒ"""
        return self.env.render(mode)
    
    def close(self):
        """å…³é—­ç¯å¢ƒ"""
        return self.env.close()
    
    @property
    def training(self):
        """æ£€æŸ¥æ˜¯å¦åœ¨è®­ç»ƒæ¨¡å¼"""
        return getattr(self, '_training', True)
    
    def train(self):
        """è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼"""
        self._training = True
    
    def eval(self):
        """è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼"""
        self._training = False

# ä½¿ç”¨ç¤ºä¾‹
def create_hasac_compatible_v2x_env(original_v2x_env):
    """
    åˆ›å»ºHASACå…¼å®¹çš„V2Xç¯å¢ƒ
    
    Args:
        original_v2x_env: åŸå§‹V2Xç¯å¢ƒ
        
    Returns:
        åŒ…è£…åçš„ç¯å¢ƒ
    """
    wrapped_env = HASACV2XWrapper(original_v2x_env)
    
    print("ğŸ¯ HASAC V2Xç¯å¢ƒåˆ›å»ºæˆåŠŸï¼")
    print("ğŸ“Š ç°åœ¨å¯ä»¥ä½¿ç”¨HASACç®—æ³•è¿›è¡Œè®­ç»ƒäº†")
    print("ğŸ”§ è¿ç»­åŠ¨ä½œä¼šè‡ªåŠ¨è½¬æ¢ä¸ºç¦»æ•£åŠ¨ä½œ")
    print("âš¡ æ”¯æŒåŠŸç‡æ§åˆ¶ã€ä¼˜å…ˆçº§è°ƒåº¦ã€è´Ÿè½½å‡è¡¡ç­‰é«˜çº§åŠŸèƒ½")
    
    return wrapped_env

if __name__ == "__main__":
    # æµ‹è¯•åŒ…è£…å™¨
    print("ğŸ§ª æµ‹è¯•HASAC V2XåŒ…è£…å™¨...")
    print("è¿™ä¸ªåŒ…è£…å™¨è®©HASACèƒ½å¤Ÿåœ¨ç°æœ‰çš„V2Xç¯å¢ƒä¸­å·¥ä½œ")
    print("æ— éœ€ä¿®æ”¹åŸå§‹ç¯å¢ƒï¼Œä¿æŒå‘åå…¼å®¹æ€§") 