# V2X Task Offloading Environment Configuration
env_name: "v2x"
scenario: "task_offloading"
num_agents: 10

# V2X Environment Parameters
env_args:
  num_agents: 10
  num_rsus: 4
  map_size: 1000.0  # meters
  max_episode_steps: 200
  
  # Vehicle Parameters
  vehicle_speed_range: [20.0, 80.0]  # km/h
  vehicle_compute_range: [1.0, 10.0]  # GHz
  vehicle_battery_range: [0.3, 1.0]   # battery level ratio
  
  # Task Parameters
  task_generation_prob: 0.3
  task_compute_range: [0.5, 5.0]     # GHz*s
  task_deadline_range: [5, 20]       # time steps
  task_data_size_range: [1.0, 50.0]  # MB
  
  # Communication Parameters
  communication_range: 300.0         # meters
  rsu_coverage: 500.0               # meters
  bandwidth: 20.0                   # MHz
  noise_power: -110.0               # dBm
  
  # Reward Parameters
  reward_task_completion: 10.0
  reward_task_failure: -5.0
  reward_energy_efficiency: 1.0
  reward_load_balance: 2.0

# Algorithm Enhancement Parameters
# Transformer Encoder Configuration (Innovation 1)
transformer_enabled: true
transformer_config:
  use_transformer: true
  max_seq_length: 50
  transformer_d_model: 256
  transformer_nhead: 8
  transformer_num_layers: 4
  transformer_dim_feedforward: 512
  transformer_dropout: 0.1

# Contrastive Learning Configuration (Innovation 1)
contrastive_learning:
  use_contrastive_learning: true
  contrastive_temperature: 0.1
  similarity_threshold: 0.8
  temporal_weight: 0.1
  lambda_cl: 0.1  # weight for contrastive loss in total loss

# Role-based Policy Configuration (Innovation 2 - Future Implementation)
role_based_policy:
  use_role_based_policy: false  # Will be enabled in innovation 2
  num_roles: 3  # Task Originator, Computational Provider, Data Relay
  role_assignment_hidden_size: 128
  kaleidoscope_enabled: false

# Standard HARL Parameters
use_centralized_V: true
use_obs_instead_of_state: true
use_render: false
use_wandb: false
use_linear_lr_decay: true
use_proper_time_limits: true
use_value_active_masks: true
use_eval: true

# Training Parameters
n_training_threads: 1
n_rollout_threads: 8
num_mini_batch: 1
episode_length: 200
num_env_steps: 2000000
ppo_epoch: 10
use_gae: true
gamma: 0.99
gae_lambda: 0.95
use_policy_active_masks: true
use_naive_recurrent_policy: false
use_recurrent_policy: true
recurrent_N: 1
hidden_size: 256
layer_N: 2
activation: "relu"
use_orthogonal: true
use_ReLU: true
use_popart: false
use_valuenorm: true
use_feature_normalization: true
use_same_share_obs: false

# Learning Rates
lr: 5e-4
critic_lr: 5e-4
opti_eps: 1e-5
weight_decay: 0

# PPO Parameters
clip_param: 0.2
num_mini_batch: 1
data_chunk_length: 10
value_loss_coef: 1
entropy_coef: 0.01
max_grad_norm: 10.0
huber_delta: 10.0

# Save and Evaluation
save_interval: 25
eval_interval: 25
eval_episodes: 32
log_interval: 5

# Device
cuda: true
cuda_deterministic: true
n_block: 1
n_embd: 64
n_head: 1
dec_actor: false
share_actor: false
